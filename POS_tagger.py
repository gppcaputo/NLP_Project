# -*- coding: utf-8 -*-
"""Untitled12.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tzk3c_51ZzXcKVeJ_1mYgBtRaCP14Z8v
"""

from google.colab import drive
drive.mount('/content/drive')

import os
drive_path = '/content/drive/MyDrive/NLP/'
file_paths = [ os.path.join(drive_path, 'data_v1.3/DevSet_EAGLES.txt'),
            os.path.join(drive_path, 'EVALITA07_POS_TestSet4RESULTS/TestSet_EAGLES.txt')]

import html
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional,TimeDistributed, SimpleRNN
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
import numpy as np
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
from gensim.models import Word2Vec

word_tag_sequences = []
for file_path in file_paths:
  with open(file_path, 'r') as file:
      sentence = []
      for line in file:
          line = line.strip().split()  # Split the line into word and tag
          if line:  # Skip empty lines
              sentence.append(tuple(line))  # Convert word-tag pair to a tuple
              if line[0] == '.':
                  word_tag_sequences.append(sentence)
                  sentence = []

X = [] # store input sequence
Y = [] # store output sequence

for sentence in word_tag_sequences:
    X_sentence = []
    Y_sentence = []
    for entity in sentence:
        X_sentence.append(html.unescape(entity[0].lower()))  # entity[0] contains the word
        Y_sentence.append(entity[1])  # entity[1] contains corresponding tag

    X.append(X_sentence)
    Y.append(Y_sentence)

num_words = len(set([word.lower() for sentence in X for word in sentence]))
num_tags   = len(set([word.lower() for sentence in Y for word in sentence]))

print("Total number of tagged sentences: {}".format(len(X)))
print("Vocabulary size: {}".format(num_words))
print("Total number of tags: {}".format(num_tags))

#check the length of first input sequence
print("Length of first input sequence  : {}".format(len(X[0])))
print("Length of first output sequence : {}".format(len(Y[0])))

# encode X

word_tokenizer = Tokenizer()                      # instantiate tokenizer
word_tokenizer.fit_on_texts(X)                    # fit tokenizer on data
X_encoded = word_tokenizer.texts_to_sequences(X)  # use the tokenizer to encode input sequence

# encode Y
tag_tokenizer = Tokenizer()
tag_tokenizer.fit_on_texts(Y)
Y_encoded = tag_tokenizer.texts_to_sequences(Y)

# look at first encoded data point

print("** Raw data point **", "\n", "-"*100, "\n")
print('X: ', X[0], '\n')
print('Y: ', Y[0], '\n')
print()
print("** Encoded data point **", "\n", "-"*100, "\n")
print('X: ', X_encoded[0], '\n')
print('Y: ', Y_encoded[0], '\n')


# make sure that each sequence of input and output is same length
different_length = [1 if len(input) != len(output) else 0 for input, output in zip(X_encoded, Y_encoded)]
print("{} sentences have disparate input-output lengths.".format(sum(different_length)))

# check length of longest sentence
lengths = [len(seq) for seq in X_encoded]
print("Length of longest sentence: {}".format(max(lengths)))


MAX_SEQ_LENGTH = 100  # sequences greater than 100 in length will be truncated

X_padded = pad_sequences(X_encoded, maxlen=MAX_SEQ_LENGTH, padding="pre", truncating="post")
Y_padded = pad_sequences(Y_encoded, maxlen=MAX_SEQ_LENGTH, padding="pre", truncating="post")

# print the first sequence
print(X_padded[0], "\n"*3)
print(Y_padded[0])

from gensim.utils import iter_windows
# Train Word2Vec model
embedding_dim = 100  # Set the desired dimensionality of the word embeddings
word2vec_model = Word2Vec(X, vector_size=embedding_dim, min_count=1, window = 5, workers =1, negative=25,sample = 1e-4)


 # each word in word2vec model is represented using a 100 dimensional vector
VOCABULARY_SIZE = len(word_tokenizer.word_index) + 1

# create an empty embedding matix
embedding_weights = np.zeros((VOCABULARY_SIZE, embedding_dim))

# create a word to index dictionary mapping
word2id = word_tokenizer.word_index

# copy vectors from word2vec model to the words present in corpus
for word, index in word2id.items():
    try:
        embedding_weights[index, :] = word2vec_model.wv[word]
    except KeyError:
        pass

print("Embeddings shape: {}".format(embedding_weights.shape))



# use Keras' to_categorical function to one-hot encode Y
Y_encoded = to_categorical(Y_padded)

# print Y of the first output sequqnce
print(Y_encoded.shape)

# split entire data into training and testing sets
TEST_SIZE = 0.15
X_train, X_test, Y_train, Y_test = train_test_split(X_padded, Y_encoded, test_size=TEST_SIZE, random_state=42)
# split training data into training and validation sets
VALID_SIZE = 0.15
X_train, X_validation, Y_train, Y_validation = train_test_split(X_train, Y_train, test_size=VALID_SIZE, random_state=42)

print("TRAINING DATA")
print('Shape of input sequences: {}'.format(X_train.shape))
print('Shape of output sequences: {}'.format(Y_train.shape))
print("-"*50)
print("VALIDATION DATA")
print('Shape of input sequences: {}'.format(X_validation.shape))
print('Shape of output sequences: {}'.format(Y_validation.shape))
print("-"*50)
print("TEST DATA")
print('Shape of input sequences: {}'.format(X_test.shape))
print('Shape of output sequences: {}'.format(Y_test.shape))

hyperparameters = {
    'lstm_units': [512],
    'dropout_rate': [0.5],
    'batch_size': [128],
    'epochs': [20]
}

# total number of tags
NUM_CLASSES = Y_encoded.shape[2]


def build_model(lstm_units, dropout_rate, batch_size, epochs):
  DNN_model = Sequential()

  # create embedding layer - usually the first layer in text problems
  DNN_model.add(Embedding(input_dim     =  VOCABULARY_SIZE,         # vocabulary size - number of unique words in data
                          output_dim    =  embedding_dim,          # length of vector with which each word is represented
                          input_length  =  MAX_SEQ_LENGTH,          # length of input sequence
                          weights       = [embedding_weights],     # word embedding matrix
                          trainable     = True
  ))
  DNN_model.add(Bidirectional(LSTM(lstm_units,
                return_sequences=True  # True - return whole sequence; False - return single output of the end of the sequence
  )))

  DNN_model.add(Dropout(dropout_rate))


  # add time distributed (output at each sequence) layer
  DNN_model.add(TimeDistributed(Dense(NUM_CLASSES, activation='softmax')))
  DNN_model.compile(loss      =  'categorical_crossentropy',
                    optimizer =  'adam',
                    metrics   =  ['acc'])
  # check summary of the model
  DNN_model.summary()
  #return DNN_model
  #training = DNN_model.fit(X_train, Y_train, batch_size = 64, epochs=15, validation_data=(X_validation, Y_validation))

  # visualise training history
  #plt.plot(training.history['acc'])c
  #plt.plot(training.history['val_acc'])
  #plt.title('model accuracy')
  #plt.ylabel('accuracy')
  #plt.xlabel('epoch')
  #plt.legend(['train', 'test'], loc="lower right")
  #plt.show()

  # Evaluate the model on the test set
  #loss, accuracy = DNN_model.evaluate(X_test, Y_test,verbose = 1)

  # Print the evaluation results
  #print("Loss:", loss)
  #print("Accuracy:", accuracy)

  return DNN_model

from itertools import product

for combo in product(*hyperparameters.values()):
    params = dict(zip(hyperparameters.keys(), combo))

    # Build and train the model
    DNN_model = build_model(**params)
    print("Start")
    training = DNN_model.fit(X_train, Y_train,  batch_size=params['batch_size'], epochs=params['epochs'], validation_data=(X_validation, Y_validation),verbose = 0)

    plt.plot(training.history['acc'])
    plt.plot(training.history['val_acc'])
    plt.title('model accuracy')
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc="lower right")
    plt.show()

    print("Evaluation")

    # Evaluate the model on the external test set
    loss, accuracy = DNN_model.evaluate(X_test, Y_test)

    Y_test_true = np.argmax(Y_test, axis=-1)
    Y_test_pred = np.argmax(DNN_model.predict(X_test), axis=-1)
    Y_test_true_flat = np.ravel(Y_test_true)
    Y_test_pred_flat = np.ravel(Y_test_pred)
    num_correct = np.sum(Y_test_true_flat == Y_test_pred_flat)
    total_tokens = Y_test_true_flat.shape[0]
    tagging_accuracy = num_correct / total_tokens


    print(num_correct)
    print(total_tokens)




    print('Test set loss:', loss)
    print('Test set accuracy:', accuracy)
    print('Tagging accuracy: ', tagging_accuracy)
    print("Configuration:")
    print(params)
    print("-------------------------------")